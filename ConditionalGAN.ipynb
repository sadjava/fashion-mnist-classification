{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional GAN for Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 64\n",
    "NUM_WORKERS = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = \".\",\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        num_workers: int = NUM_WORKERS,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dims = (1, 28, 28)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        FashionMNIST(self.data_dir, train=True, download=True)\n",
    "        FashionMNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            fmnist_full = FashionMNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.fmnist_train, self.fmnist_val = random_split(fmnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.fmnist_test = FashionMNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.fmnist_train,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.fmnist_val, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.fmnist_test, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim=10, im_chan=1, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.gen = nn.Sequential(\n",
    "            self.make_gen_block(input_dim, hidden_dim * 4, stride=1, padding=0),\n",
    "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=3),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
    "            self.make_gen_block(hidden_dim, im_chan, final_layer=True),\n",
    "        )\n",
    "    \n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size=4, stride=2, padding=1, final_layer=False):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "    \n",
    "    def forward(self, noise):\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "    \n",
    "def get_noise(n_samples, input_dim):\n",
    "    return torch.randn(n_samples, input_dim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, im_chan=1, hidden_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self.make_disc_block(im_chan, hidden_dim),\n",
    "            self.make_disc_block(hidden_dim, hidden_dim * 2),\n",
    "            self.make_disc_block(hidden_dim * 2, hidden_dim * 4, kernel_size=3),\n",
    "            self.make_disc_block(hidden_dim * 4, 1, stride=1, padding=0, final_layer=True),\n",
    "        )\n",
    "    \n",
    "    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, padding=1, final_layer=False):\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "    \n",
    "    def forward(self, image):\n",
    "        disc_pred = self.disc(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dimensions(z_dim, mnist_shape, n_classes):\n",
    "    generator_input_dim = z_dim + n_classes\n",
    "    discriminator_im_chan = mnist_shape[0] + n_classes\n",
    "    return generator_input_dim, discriminator_im_chan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN (LightningModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        width,\n",
    "        height,\n",
    "        n_classes,\n",
    "        latent_dim: int = 100,\n",
    "        lr: float = 0.0002,\n",
    "        b1: float = 0.5,\n",
    "        b2: float = 0.999,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        # networks\n",
    "        generator_input_dim, discriminator_im_chan = get_input_dimensions(latent_dim, (channels, width, height), n_classes)\n",
    "        data_shape = (channels, width, height)\n",
    "        self.generator = Generator(input_dim=generator_input_dim)\n",
    "        self.discriminator = Discriminator(im_chan=discriminator_im_chan)\n",
    "\n",
    "        def weights_init(m):\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.generator = self.generator.apply(weights_init)\n",
    "        self.discriminator = self.discriminator.apply(weights_init)\n",
    "\n",
    "        self.validation_z = torch.cat((torch.randn(10, latent_dim), F.one_hot(torch.arange(n_classes))), 1)\n",
    "        \n",
    "\n",
    "        self.example_input_array = torch.zeros(2, generator_input_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        imgs, labels = batch\n",
    "        optimizer_g, optimizer_d = self.optimizers()\n",
    "\n",
    "        # sample noise and one hot labels\n",
    "        one_hot_labels = F.one_hot(labels, self.hparams.n_classes)\n",
    "        one_hot_labels = one_hot_labels.type_as(imgs)\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = image_one_hot_labels.repeat(1, 1, self.hparams.height, self.hparams.width).type_as(imgs)\n",
    "        \n",
    "        fake_noise = get_noise(imgs.shape[0], self.hparams.latent_dim).type_as(imgs)\n",
    "        noise_and_labels = torch.cat((fake_noise, one_hot_labels), 1).type_as(imgs)\n",
    "        \n",
    "        # train generator and generate images\n",
    "        self.toggle_optimizer(optimizer_g, optimizer_idx=0)\n",
    "\n",
    "        fake = self.generator.forward(noise_and_labels)\n",
    "\n",
    "        # adversarial loss for generator\n",
    "        fake_images_and_labels = torch.cat((fake, image_one_hot_labels), 1).type_as(imgs)\n",
    "        disc_fake_pred = self.discriminator.forward(fake_images_and_labels).type_as(imgs)\n",
    "        valid = torch.ones_like(disc_fake_pred).type_as(imgs)\n",
    "        g_loss = self.adversarial_loss(disc_fake_pred, valid)\n",
    "        self.manual_backward(g_loss)\n",
    "        optimizer_g.step()\n",
    "        optimizer_g.zero_grad()\n",
    "        self.log(\"g_loss\", g_loss, prog_bar=True)\n",
    "        self.untoggle_optimizer(optimizer_g)\n",
    "\n",
    "        # train discriminator\n",
    "        self.toggle_optimizer(optimizer_d, optimizer_idx=1)\n",
    "\n",
    "        # distinguish fake images\n",
    "        fake_images_and_labels = torch.cat((fake, image_one_hot_labels), 1).type_as(imgs)\n",
    "        disc_fake_pred = self.discriminator.forward(fake_images_and_labels.detach()).type_as(imgs)\n",
    "        valid = torch.zeros_like(disc_fake_pred).type_as(imgs)\n",
    "        fake_loss = self.adversarial_loss(disc_fake_pred, valid)\n",
    "        \n",
    "        # distinguish real images\n",
    "        real_images_and_labels = torch.cat((imgs, image_one_hot_labels), 1).type_as(imgs)\n",
    "        disc_real_pred = self.discriminator.forward(real_images_and_labels).type_as(imgs)\n",
    "        valid = torch.ones_like(disc_real_pred).type_as(imgs)\n",
    "        real_loss = self.adversarial_loss(disc_real_pred, valid)\n",
    "\n",
    "        # ground truth results\n",
    "        d_loss = (fake_loss + real_loss) / 2\n",
    "        self.log(\"d_loss\", d_loss, prog_bar=True)\n",
    "        self.manual_backward(d_loss)\n",
    "        optimizer_d.step()\n",
    "        optimizer_d.zero_grad()\n",
    "        self.untoggle_optimizer(optimizer_d)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        z = self.validation_z.type_as(self.generator.gen[0][0].weight)\n",
    "\n",
    "        # log sampled images\n",
    "        sample_imgs = self(z)\n",
    "        grid = torchvision.utils.make_grid(sample_imgs, nrow=2)\n",
    "        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"gan_logs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = FMNISTDataModule()\n",
    "dm.prepare_data()\n",
    "dm.setup(\"fit\")\n",
    "model = GAN(*dm.dims, dm.num_classes)\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=100,\n",
    "    logger=logger,\n",
    ")\n",
    "trainer.fit(model, dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
